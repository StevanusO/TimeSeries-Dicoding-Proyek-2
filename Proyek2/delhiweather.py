# -*- coding: utf-8 -*-
"""DelhiWeather

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RG_Yi7tKc9HmMy9tFbZfT-r_z5EoU0-j
"""

from google.colab import files
#Upload Kaggle API Key
files.upload()

!mkdir ~/.kaggle #make new directory in root folder
!cp kaggle.json ~/.kaggle/ #copy and paste kaggle API key to new directory
!chmod 600 ~/.kaggle/kaggle.json #permission
!kaggle datasets list

!kaggle datasets download -d mahirkukreja/delhi-weather-data

import os
import pandas as pd
import zipfile

zipPath = '../content/delhi-weather-data.zip'
zipFile = zipfile.ZipFile(zipPath, 'r')
zipFile.extractall('../content/delhiWeatherDataset') 
zipFile.close()

df = pd.read_csv('../content/delhiWeatherDataset/testset.csv')
df.head()

df.isnull().sum()

df_1 = df[["datetime_utc"," _tempm"]]

df_1.columns =["date", 'temp']

df_1.isnull().sum()

df_1.dropna(inplace = True)

df_1["date"] = pd.to_datetime(df_1["date"])
df_1.info()

df_1 = df_1.set_index('date')

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range = (0, 1))
data_1 = scaler.fit_transform(df_1)

import matplotlib.pyplot as plt
plt.plot(data_1)

train_size = int(len(data_1)*0.8)
test_size = len(data_1) - train_size
X_data, y_data = data_1[0: train_size, :], data_1[train_size: len(data_1), :1]
train_size, test_size

import tensorflow as tf
import numpy as np
def dataset(dataset, time_step=1):
    X, Y = [], []
    for i in range(len(dataset)-time_step-1):
        a = dataset[i:(i+time_step), 0]   
        X.append(a)
        Y.append(dataset[i + time_step, 0])
    return np.array(X), np.array(Y)

X_train, y_train = dataset(X_data, 100)
X_test, y_test = dataset(y_data, 100)

X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Bidirectional, Dropout

model = Sequential([
    LSTM(64, return_sequences = True, input_shape=(100, 1)),
    Dropout(0.1),
    LSTM(64, return_sequences = True),
    Dropout(0.1),
    Bidirectional(LSTM(64)),
    Dropout(0.1),
    Dense(8, activation = 'relu'),
    Dense(1)
])

from tensorflow.keras.callbacks import EarlyStopping
auto_stop_learn = EarlyStopping(
    monitor = 'loss',
    min_delta = 0,
    patience = 2,
    verbose = 1,
    mode = 'auto' 
)

optimizer = tf.keras.optimizers.SGD(learning_rate=1.0000e-04, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"]
)

history = model.fit(
    X_train, y_train,
    validation_data = (X_test, y_test),
    epochs = 5,
    batch_size = 128, 
    callbacks = auto_stop_learn,
    verbose = 1
)

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.legend(['train', 'test'], loc = 'upper right')
plt.show()

plt.plot(history.history['mae'])
plt.plot(history.history['val_mae'])
plt.title('Model Mae')
plt.ylabel('Mae')
plt.xlabel('Epoch')
plt.legend(['train', 'test'], loc='lower right')
plt.show()